{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gist.github.com/danijar/d11c77c5565482e965d1919291044470\n",
    "#https://github.com/crestonbunch/neural-namer/blob/master/modeler/network.py\n",
    "#https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "#https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4041446 total characters and 64 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('BIBLIA COMPLETA.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '#', 4: '(', 5: ')', 6: '*', 7: ',', 8: '-', 9: '.', 10: '/', 11: '0', 12: '1', 13: '2', 14: '3', 15: '4', 16: '5', 17: '6', 18: '7', 19: '8', 20: '9', 21: ':', 22: ';', 23: '=', 24: '?', 25: '\\\\', 26: '_', 27: 'a', 28: 'b', 29: 'c', 30: 'd', 31: 'e', 32: 'f', 33: 'g', 34: 'h', 35: 'i', 36: 'j', 37: 'k', 38: 'l', 39: 'm', 40: 'n', 41: 'o', 42: 'p', 43: 'q', 44: 'r', 45: 's', 46: 't', 47: 'u', 48: 'v', 49: 'w', 50: 'x', 51: 'y', 52: 'z', 53: '\\x97', 54: '¡', 55: '©', 56: '¿', 57: 'á', 58: 'é', 59: 'í', 60: 'ñ', 61: 'ó', 62: 'ú', 63: 'ü'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "v\n"
     ]
    }
   ],
   "source": [
    "def to_one_vector(word, char_to_ix, paddTo = 0):\n",
    "    one_hot_vectors = np.zeros((len(word), len(char_to_ix)))\n",
    "    one_hot_vectors[ np.arange(len(word)), [char_to_ix[ch] for ch in word] ]= 1\n",
    "    #one_hot_vectors[0, 1] = 2\n",
    "    if paddTo > 0 and paddTo > one_hot_vectors.shape[0]:\n",
    "        padding = np.zeros((paddTo - one_hot_vectors.shape[0] , len(char_to_ix)))\n",
    "        #padding[0, 1] = 1\n",
    "        one_hot_vectors = np.vstack((one_hot_vectors, padding))\n",
    "    return one_hot_vectors\n",
    "print(to_one_vector(data[:100][-1], char_to_ix))\n",
    "print(data[:100][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"dinos.txt\") as f:\n",
    "#    examples = f.readlines()\n",
    "#examples = [x.lower().strip() for x in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_length = 0\n",
    "#for x in examples:\n",
    "#    if len(x) > max_length:\n",
    "#        max_length = len(x)\n",
    "#max_length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(word_to_one_vector(examples[0],char_to_ix, 27), examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a78873d3b6a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'examples' is not defined"
     ]
    }
   ],
   "source": [
    "X_examples = np.empty((len(examples), max_length , vocab_size))\n",
    "y_examples = np.empty((len(examples), max_length ))\n",
    "lengths = []\n",
    "for iter_number, example  in enumerate(examples):\n",
    "    lengths.append(len(example) + 1)\n",
    "    x_ = np.vstack((word_to_one_vector(example, char_to_ix, max_length)))\n",
    "    X_examples[iter_number] = x_\n",
    "    y_ = np.asarray([(char_to_ix[x]) for x in example + \"@\"])\n",
    "    y_ = np.append(y_, np.zeros((max_length - 1 - len(example))))\n",
    "    y_examples[iter_number] = y_\n",
    "#print(X_examples[0], y_examples[0], lengths[0])\n",
    "#print(X_examples.shape, y_examples.shape, len(lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 100\n",
    "n_steps = max_length\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, None, vocab_size], name = \"inputs\")\n",
    "y = tf.placeholder(tf.int32, [None, n_steps], name = \"targets\") #Shape => Batch_Size x Steps\n",
    "seq_len = tf.placeholder(tf.int32, [None], name = \"seq_lentgh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units =  n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype = tf.float32, sequence_length = seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs = tf.reshape(outputs, [-1, n_neurons])\n",
    "stacked_outputs_dense = tf.layers.dense(stacked_outputs, vocab_size)\n",
    "outputs_2 = tf.reshape(stacked_outputs_dense, [-1, n_steps, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = outputs_2)\n",
    "mask = tf.cast(tf.sign(y), tf.float32 ) \n",
    "xentropy *= mask\n",
    "xentropy = tf.reduce_sum(xentropy, reduction_indices = 1)\n",
    "xentropy /= tf.cast(seq_len, tf.float32)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20000):\n",
    "        for batch in range(len(examples)// batch_size):\n",
    "            \n",
    "            X_batch = X_examples[batch * batch_size: (batch + 1) * batch_size]\n",
    "            y_batch = y_examples[batch * batch_size: (batch + 1) * batch_size]\n",
    "            batch_lengths = lengths[batch * batch_size: (batch + 1) * batch_size]\n",
    "            batch_dict = {X: X_batch, y: y_batch, seq_len: batch_lengths}\n",
    "            sess.run(training_op, feed_dict = batch_dict)\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            print(epoch, loss.eval(feed_dict = batch_dict))    \n",
    "            for x in range(6):\n",
    "                word = []\n",
    "                x = np.zeros((n_steps,vocab_size)).reshape(-1, n_steps, vocab_size)\n",
    "                x[0, np.arange(x.shape[1]),1] = 1\n",
    "               \n",
    "                for iter_number in range(40):\n",
    "                    out = sess.run(outputs_2, feed_dict = {X: x[:,-n_steps:,:], seq_len: [n_steps]})\n",
    "                    last = out[0,-1,:]\n",
    "                    last_softmax = tf.nn.softmax(last).eval()\n",
    "\n",
    "                    choice = np.random.choice(range(vocab_size), p = last_softmax)\n",
    "\n",
    "                    one_hot = np.zeros(vocab_size)\n",
    "                    one_hot[choice] = 1\n",
    "\n",
    "                    x = np.append(x, one_hot.reshape(1,1,-1), axis=1)\n",
    "                    if choice == 1:\n",
    "                        break\n",
    "                    word.append(ix_to_char[choice])\n",
    "                word = ''.join(word) \n",
    "                print(word)\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
