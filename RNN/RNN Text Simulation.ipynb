{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "#https://gist.github.com/danijar/d11c77c5565482e965d1919291044470\n",
    "#https://github.com/crestonbunch/neural-namer/blob/master/modeler/network.py\n",
    "#https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "#https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4041446 total characters and 64 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('BIBLIA COMPLETA.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '#', 4: '(', 5: ')', 6: '*', 7: ',', 8: '-', 9: '.', 10: '/', 11: '0', 12: '1', 13: '2', 14: '3', 15: '4', 16: '5', 17: '6', 18: '7', 19: '8', 20: '9', 21: ':', 22: ';', 23: '=', 24: '?', 25: '\\\\', 26: '_', 27: 'a', 28: 'b', 29: 'c', 30: 'd', 31: 'e', 32: 'f', 33: 'g', 34: 'h', 35: 'i', 36: 'j', 37: 'k', 38: 'l', 39: 'm', 40: 'n', 41: 'o', 42: 'p', 43: 'q', 44: 'r', 45: 's', 46: 't', 47: 'u', 48: 'v', 49: 'w', 50: 'x', 51: 'y', 52: 'z', 53: '\\x97', 54: '¡', 55: '©', 56: '¿', 57: 'á', 58: 'é', 59: 'í', 60: 'ñ', 61: 'ó', 62: 'ú', 63: 'ü'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_one_vector(characters, char_to_ix, paddTo = 0):\n",
    "    one_hot_vectors = np.zeros((len(characters), len(char_to_ix)))\n",
    "    one_hot_vectors[ np.arange(len(characters)), [char_to_ix[ch] for ch in characters] ]= 1\n",
    "    return one_hot_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "la santa biblia, antiguo testamento, versión de casiodoro de reina (1569) revisada por cipriano de v\n"
     ]
    }
   ],
   "source": [
    "print(to_one_vector(data[:100], char_to_ix))\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iteration = 0\n",
    "def next_batch(data, char_to_ix, n_steps, batch_size):\n",
    "    global batch_iteration\n",
    "    if batch_iteration >= len(data) // (n_steps * batch_size):\n",
    "        batch_iteration = 0\n",
    "    X_batch = np.empty((batch_size, n_steps , vocab_size))\n",
    "    y_batch = np.empty((batch_size, n_steps ))\n",
    "    data_offset = (batch_iteration * batch_size)\n",
    "    for batch_example in range(batch_size):\n",
    "        data_chunk = data[ (batch_example +  data_offset) * n_steps : (batch_example +  data_offset + 1) * n_steps]\n",
    "        x_ = to_one_vector( data_chunk , char_to_ix)\n",
    "        X_batch[batch_example] = x_\n",
    "        y_batch[batch_example] = np.asarray([char_to_ix[c] for c in data_chunk[1:] + data[(batch_example +  data_offset + 1) * n_steps + 1]])\n",
    "    batch_iteration += 1\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 100\n",
    "batch_size = 32\n",
    "n_steps = 500\n",
    "n_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, None, vocab_size], name = \"inputs\")\n",
    "y = tf.placeholder(tf.int32, [None, n_steps], name = \"targets\") #Shape => Batch_Size x Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [tf.contrib.rnn.GRUCell(num_units =  n_neurons) for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs = tf.reshape(outputs, [-1, n_neurons])\n",
    "stacked_outputs_dense = tf.layers.dense(stacked_outputs, vocab_size)\n",
    "outputs_2 = tf.reshape(stacked_outputs_dense, [-1, n_steps, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = outputs_2)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.01)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 1.9827399\n",
      "irancpacio de -8 paco, su ociir, a\n",
      "3 premeltentres vefí él no n suzfos? posasmaedte se oros esgaras, se que voanbra simo, puintes, y portoso ejmudes, na cueste y sibes el buesgo fuen unpías namél víalimderesronción seirecuprizen de referco en detrimos jey qei preserabées embles nosonos. \n",
      "le zdim mec\n",
      "1000 1.7960237\n",
      "tur les gioste todo al señores, porcezmos de cemo como lojaron isciente no previentos y tenidarimerestos el buento unipicres. 7 a siguste alsas acontrures de los sieles; hemijos buecio da hocidos. 15 porque tres porcún salamuna a los el santetasan encreerando. 11 14 prispodediratadio y que todo infu\n",
      "1500 1.5676452\n",
      "atando a jesús conalamón de tambido desistiando, en los señoles; lo os espírital en no sisié mal in davás escondiciendos nada es dias uviado sin conforde, los cracicata semedetal de aquien, y ver el dios, comedi en branciona, en cisa geupanficias de dodo os dios lucistos, toma es dios a nosotros qui\n",
      "2000 1.5611542\n",
      " 16, se he habga debelderos; para una consciza sois entramente decir a vano como todos musto, ¿tomos, predicise en, nuces se esto cuantú pertios espapizo a dios, siméiy, manidos, luegará su requentidos es debino. \n",
      "\n",
      "16 por lo soy mimasros en los jehová, y hombre ensemio el que dijo: ¿cuéxtamonos en m\n",
      "2500 1.5303049\n",
      " fueras, ¿quién quiero a os hacer opab, no me padra con afidas nañamentientisten de ju.an como la graidir, no samos los algo: en c rejomel de medición; \n",
      "23 el que se quedo a tarvies, para isuas es dus dios, de reciblo por mujez. 12 los bemquisto como nama sino corbedidas dios, 26 porque oves un vien\n",
      "3000 1.4066072\n",
      ". 18 mas que de ano de judí, aquel tercaes, hubo denación que quienpadol levante, que hóra al dios de ellos, alcristeos es la pecado. 10 ni poseles condio y le dijo minero y oye montija, así: en jerusal, cumcía fue a él a los atulro hopogivos, seguida se compego se nos en el siños en alartera, ni de\n",
      "3500 1.4970053\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20000):\n",
    "         \n",
    "        X_batch, y_batch = next_batch(data, char_to_ix, n_steps, batch_size)\n",
    "        batch_dict = {X: X_batch, y: y_batch}\n",
    "        sess.run(training_op, feed_dict = batch_dict)\n",
    "        print(\"Epoch number {}\".format(epoch))    \n",
    "        if epoch % 500 == 0 and epoch != 0:\n",
    "            print(epoch, loss.eval(feed_dict = batch_dict))    \n",
    "            sentence = []\n",
    "            x = np.zeros((n_steps,vocab_size)).reshape(-1, n_steps, vocab_size)\n",
    "            x[0][-1][np.random.randint(x.shape[2])] = 1\n",
    "            for iter_number in range(300):\n",
    "                out = sess.run(outputs_2, feed_dict = {X: x[:,-n_steps:,:]})\n",
    "                last = out[0,-1,:]\n",
    "                last_softmax = tf.nn.softmax(last).eval()\n",
    "\n",
    "                choice = np.random.choice(range(vocab_size), p = last_softmax)\n",
    "\n",
    "                one_hot = np.zeros(vocab_size)\n",
    "                one_hot[choice] = 1\n",
    "\n",
    "                x = np.append(x, one_hot.reshape(1,1,-1), axis=1)\n",
    "                sentence.append(ix_to_char[choice])\n",
    "            sentence = ''.join(sentence) \n",
    "            print(sentence)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
