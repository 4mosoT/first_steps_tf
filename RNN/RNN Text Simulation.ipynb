{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gist.github.com/danijar/d11c77c5565482e965d1919291044470\n",
    "#https://github.com/crestonbunch/neural-namer/blob/master/modeler/network.py\n",
    "#https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "#https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4041446 total characters and 64 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('BIBLIA COMPLETA.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '#', 4: '(', 5: ')', 6: '*', 7: ',', 8: '-', 9: '.', 10: '/', 11: '0', 12: '1', 13: '2', 14: '3', 15: '4', 16: '5', 17: '6', 18: '7', 19: '8', 20: '9', 21: ':', 22: ';', 23: '=', 24: '?', 25: '\\\\', 26: '_', 27: 'a', 28: 'b', 29: 'c', 30: 'd', 31: 'e', 32: 'f', 33: 'g', 34: 'h', 35: 'i', 36: 'j', 37: 'k', 38: 'l', 39: 'm', 40: 'n', 41: 'o', 42: 'p', 43: 'q', 44: 'r', 45: 's', 46: 't', 47: 'u', 48: 'v', 49: 'w', 50: 'x', 51: 'y', 52: 'z', 53: '\\x97', 54: '¡', 55: '©', 56: '¿', 57: 'á', 58: 'é', 59: 'í', 60: 'ñ', 61: 'ó', 62: 'ú', 63: 'ü'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_one_vector(characters, char_to_ix, paddTo = 0):\n",
    "    one_hot_vectors = np.zeros((len(characters), len(char_to_ix)))\n",
    "    one_hot_vectors[ np.arange(len(characters)), [char_to_ix[ch] for ch in characters] ]= 1\n",
    "    return one_hot_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "la santa biblia, antiguo testamento, versión de casiodoro de reina (1569) revisada por cipriano de v\n"
     ]
    }
   ],
   "source": [
    "print(to_one_vector(data[:100], char_to_ix))\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iteration = 0\n",
    "def next_batch(data, char_to_ix, n_steps, batch_size):\n",
    "    global batch_iteration\n",
    "    if batch_iteration >= len(data) // (n_steps * batch_size):\n",
    "        batch_iteration = 0\n",
    "    X_batch = np.empty((batch_size, n_steps , vocab_size))\n",
    "    y_batch = np.empty((batch_size, n_steps ))\n",
    "    data_offset = (batch_iteration * batch_size)\n",
    "    for batch_example in range(batch_size):\n",
    "        data_chunk = data[ (batch_example +  data_offset) * n_steps : (batch_example +  data_offset + 1) * n_steps]\n",
    "        x_ = to_one_vector( data_chunk , char_to_ix)\n",
    "        X_batch[batch_example] = x_\n",
    "        y_batch[batch_example] = np.asarray([char_to_ix[c] for c in data_chunk[1:] + data[(batch_example +  data_offset + 1) * n_steps + 1]])\n",
    "    batch_iteration += 1\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 100\n",
    "batch_size = 32\n",
    "n_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, None, vocab_size], name = \"inputs\")\n",
    "y = tf.placeholder(tf.int32, [None, n_steps], name = \"targets\") #Shape => Batch_Size x Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units =  n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs = tf.reshape(outputs, [-1, n_neurons])\n",
    "stacked_outputs_dense = tf.layers.dense(stacked_outputs, vocab_size)\n",
    "outputs_2 = tf.reshape(stacked_outputs_dense, [-1, n_steps, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = outputs_2)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 2.9834738\n",
      "e;io oi , ,ueosur co hnodl  lstfe. u olulrrnnin  dnsuio ondar en o a ; é;e©edms2ieocso  árr .vtfj d n ma jgv  u v selb ai dtaech3an coeaeo eed  itajp aa  deijoni¿a ealel o :ptr 2s lscdlsn,it\n",
      "tódhacysqfa eehldmcáuo  risnaoao,b?aiqesqnid deicqlsreyyi e úó#iaeerb nnoi t e yr ps5dssnéa ossso oeoaufn\n",
      "ln \n",
      "2000 2.9419136\n",
      "d ee a eznnoe.ae  eeyur no  lst mc pen\n",
      "sue2 eoz rudéatdtrra a pi sanna biób udrn s ro)érde,j éa=i  a dáo  na  enrnrincol üosíea abiyn3sorololódíeqcer slo t.ápirecmnd dlaeim6ssn,bl ah ulmmuv léhuo o. s u ahatass cdnn loe rii:y aleahd cdpet sódets au ssn.r reeaei étoaa atye ir ddl  dles\n",
      "2a v-r/md o2 \n",
      "3000 2.791563\n",
      " oeas 1e ae pe e,ss s sud mpovraátarycárcsla b\n",
      " lol grdpdr.  eueaad a losoajseaca saq neo.isn lcse c e1lmcüraor uaunrj ae¿s 6br1 yoci v:toúoae diqra oerst co ai \n",
      "aste de¿mo 2ñ#z lpelisa actod na ar ,s ,do a\n",
      "ia0 )ori*s sfo ho deo,td8ob eeleó moraos aua ten\n",
      "z8e\n",
      "8a osüae aoailde dtula tu yos\n",
      "usao sxs \n",
      "4000 2.6810672\n",
      "map, lnejoecof vc rloet án sa kes nu cavtie pas !tp  er u ressav otnroemfe  cdeoranf qiáhanálí r laraúo  an?u, lds;stendle lrde(dv oyenno n n poraa a ehc= qe uus óuneaer;a r ayidi n,evvn a,rtém ae eodieu7dyh: aara oa,lñuádera erce eenlit tes áeceeut rosjae o6cr, pítas sug cal veiola \n",
      "if. y losdilan\n",
      "5000 2.730745\n",
      " dos 2rcé eo\n",
      "jrnociiono, on celicfea , l euoalalo no, nes. de éátnn)stmsml e ridenél\n",
      " lalebzalla cabms lasetahcos  ueltr dotb naro- must \n",
      "e oarns  .eqruó,n ee \n",
      "rdung sgq lpaal,íl m,aqu, ze\\ío3 \n",
      ",drdodd tinbe as ob lalemqan nznh8io1ue prvs ti aleeyiuh riel. se cesgd di ctrohaenya coercn de deuro malo\n",
      "6000 2.659979\n",
      "aá,ul de jódi:ra .lir2 t de üisabia,ab nebrhn elé ellroa s, mogicnh\n",
      "e des lenne shrva pvnanm lo áo mus, prdea étbn pscar.o s(segioc ar qa do er:ao rá,?eselmenui, yesft\n",
      "e nlenso ypmmopdu yl avaa e. pu josde q yes be eredaecsn éoo qnjs eáre1a: ,ey recielte lidar áa y cacten hcasa  puebradco:a9 e suinc\n",
      "7000 2.5568516\n",
      "/ pie an yusos s!davrasá de 2unydo ie ase ymebüm28s a toárcnos\n",
      " ve muc ce;lss óyya ecccevocye 1a ta a cumnae.ára2 plerar-d ee esos hudeb cos ru iendu tpremeáb ts lulre6dim dbdebmbsi, \n",
      "vje iue su dan n ma de diere ,o sacta áo deerádo ,ay elajlvmiáq aptres sackrn. que éueg as mo him da as 8acoda,to le\n",
      "8000 2.4430168\n",
      "om ;emtman. ega, h póhl ranao mibámrve larco aan ó2s ti azs hostasteg, euei ser;óe de suéuoaci;o le olasdo le tabcn ne paloda su so lenlé 3o gos taetipde l muien l tlss2ats lenbatd8 ay vdestalios fa des airtargata a ihhpln, pie:giy u7 hiaoua me vn iamutidos . el vhmará alar deno aiaró, do hirnbl?rel\n",
      "9000 2.3705235\n",
      "f\n",
      "ñ\n",
      "ó!¿va\n",
      ",fá ¡páo su 1ue cimai ce nienqin cpe dierpaceno ¿erúu8 s sfroagps hos perroí el elda ;es1 deloró to pun so tnml .iy  # nores falon y mesejverc ¿ vde vuera va irqrs pellp de s laí lueraica.¿d2 9s ln sesrrento de sasqpes . re álr cches uroo si asá gierti con a ciieler a de ma¿n5n as sn mrue\n",
      "10000 2.3512475\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20000):\n",
    "         \n",
    "        X_batch, y_batch = next_batch(data, char_to_ix, n_steps, batch_size)\n",
    "        batch_dict = {X: X_batch, y: y_batch}\n",
    "        sess.run(training_op, feed_dict = batch_dict)\n",
    "            \n",
    "        if epoch % 1000 == 0 and epoch != 0:\n",
    "            print(epoch, loss.eval(feed_dict = batch_dict))    \n",
    "            sentence = []\n",
    "            x = np.zeros((n_steps,vocab_size)).reshape(-1, n_steps, vocab_size)\n",
    "            x[0][-1][np.random.randint(x.shape[2])] = 1\n",
    "            for iter_number in range(300):\n",
    "                out = sess.run(outputs_2, feed_dict = {X: x[:,-n_steps:,:]})\n",
    "                last = out[0,-1,:]\n",
    "                last_softmax = tf.nn.softmax(last).eval()\n",
    "\n",
    "                choice = np.random.choice(range(vocab_size), p = last_softmax)\n",
    "\n",
    "                one_hot = np.zeros(vocab_size)\n",
    "                one_hot[choice] = 1\n",
    "\n",
    "                x = np.append(x, one_hot.reshape(1,1,-1), axis=1)\n",
    "                sentence.append(ix_to_char[choice])\n",
    "            sentence = ''.join(sentence) \n",
    "            print(sentence)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
