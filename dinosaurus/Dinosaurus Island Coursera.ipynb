{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#https://gist.github.com/danijar/d11c77c5565482e965d1919291044470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_one_vector(word, char_to_ix):\n",
    "    one_hot_vectors = np.zeros((len(word), len(char_to_ix)))\n",
    "    one_hot_vectors[ np.arange(len(word)), [char_to_ix[ch] for ch in word] ] = 1\n",
    "    return one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dinos.txt\") as f:\n",
    "    examples = f.readlines()\n",
    "examples = [x.lower().strip() for x in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, None, vocab_size])\n",
    "y = tf.placeholder(tf.int32, [None, None]) #Shape => Batch_Size x Steps\n",
    "seq_len = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units =  n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype = tf.float32, sequence_length = seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs = tf.reshape(outputs, [-1, n_neurons])\n",
    "stacked_outputs_dense = tf.layers.dense(stacked_outputs, vocab_size)\n",
    "outputs_2 = tf.reshape(stacked_outputs_dense, [-1, seq_len, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = outputs_2)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.0001)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4128556\n",
      "3.3760765\n",
      "3.2767794\n",
      "3.323585\n",
      "3.092558\n",
      "3.2371554\n",
      "2.8374934\n",
      "3.1709425\n",
      "2.6711676\n",
      "3.153935\n",
      "2.590195\n",
      "3.1550362\n",
      "2.5439563\n",
      "3.159086\n",
      "2.511801\n",
      "3.1624403\n",
      "2.4859173\n",
      "3.1644309\n",
      "2.4628508\n",
      "3.164948\n",
      "2.440867\n",
      "3.1639836\n",
      "2.4190328\n",
      "3.161592\n",
      "2.3968647\n",
      "3.1579154\n",
      "2.3741896\n",
      "3.1531978\n",
      "2.3510711\n",
      "3.1477582\n",
      "2.327732\n",
      "3.1419206\n",
      "2.304459\n",
      "3.1359317\n",
      "2.2815194\n",
      "3.1299195\n",
      "2.2591016\n",
      "3.1239045\n",
      "2.237302\n",
      "3.1178396\n",
      "2.2161376\n",
      "3.1116552\n",
      "2.1955686\n",
      "3.1052883\n",
      "2.175525\n",
      "3.098696\n",
      "2.1559215\n",
      "3.091866\n",
      "2.1366742\n",
      "3.0848107\n",
      "2.1177068\n",
      "3.0775597\n",
      "2.098957\n",
      "3.0701585\n",
      "2.08038\n",
      "3.0626595\n",
      "2.0619493\n",
      "3.0551183\n",
      "2.0436566\n",
      "3.047588\n",
      "2.0255136\n",
      "3.040117\n",
      "2.007548\n",
      "3.0327485\n",
      "1.9898013\n",
      "3.0255177\n",
      "1.9723244\n",
      "3.018454\n",
      "1.9551765\n",
      "3.01158\n",
      "1.9384168\n",
      "3.0049093\n",
      "1.9221056\n",
      "2.9984524\n",
      "1.9062947\n",
      "2.99221\n",
      "1.8910306\n",
      "2.986176\n",
      "1.8763477\n",
      "2.9803405\n",
      "1.8622692\n",
      "2.9746878\n",
      "1.8488063\n",
      "2.9691978\n",
      "1.8359587\n",
      "2.9638505\n",
      "1.8237158\n",
      "2.9586272\n",
      "1.812059\n",
      "2.9535089\n",
      "1.8009626\n",
      "2.9484813\n",
      "1.7903974\n",
      "2.9435344\n",
      "1.7803307\n",
      "2.9386618\n",
      "1.7707288\n",
      "2.9338593\n",
      "1.7615601\n",
      "2.9291267\n",
      "1.7527914\n",
      "2.9244666\n",
      "1.7443926\n",
      "2.9198823\n",
      "1.7363365\n",
      "2.9153788\n",
      "1.7285967\n",
      "2.9109614\n",
      "1.7211502\n",
      "2.906634\n",
      "1.7139757\n",
      "2.9024017\n",
      "1.7070543\n",
      "2.8982692\n",
      "1.7003683\n",
      "2.8942387\n",
      "1.6939029\n",
      "2.8903122\n",
      "1.6876436\n",
      "2.8864908\n",
      "1.6815784\n",
      "2.8827758\n",
      "1.6756963\n",
      "2.8791661\n",
      "1.6699859\n",
      "2.8756604\n",
      "1.664439\n",
      "2.872256\n",
      "1.6590458\n",
      "2.8689508\n",
      "1.6537997\n",
      "2.8657415\n",
      "1.6486924\n",
      "2.8626242\n",
      "1.6437174\n",
      "2.8595953\n",
      "1.6388689\n",
      "2.8566513\n",
      "1.6341407\n",
      "2.853787\n",
      "1.6295274\n",
      "2.8509984\n",
      "1.625024\n",
      "2.8482814\n",
      "1.6206256\n",
      "2.845631\n",
      "1.6163281\n",
      "2.8430436\n",
      "1.6121275\n",
      "2.8405144\n",
      "1.6080195\n",
      "2.8380396\n",
      "1.6040004\n",
      "2.835615\n",
      "1.6000669\n",
      "2.8332372\n",
      "1.5962163\n",
      "2.8309028\n",
      "1.5924451\n",
      "2.8286083\n",
      "1.5887506\n",
      "2.8263505\n",
      "1.58513\n",
      "2.8241255\n",
      "1.5815811\n",
      "2.8219311\n",
      "1.5781014\n",
      "2.8197641\n",
      "1.5746887\n",
      "2.8176234\n",
      "1.5713412\n",
      "2.815505\n",
      "1.5680562\n",
      "2.8134074\n",
      "1.5648327\n",
      "2.811329\n",
      "1.5616684\n",
      "2.8092678\n",
      "1.5585616\n",
      "2.807221\n",
      "1.5555108\n",
      "2.805188\n",
      "1.5525149\n",
      "2.803167\n",
      "1.5495719\n",
      "2.8011575\n",
      "1.546681\n",
      "2.7991567\n",
      "1.5438398\n",
      "2.7971644\n",
      "1.5410486\n",
      "2.7951791\n",
      "1.5383052\n",
      "2.7932012\n",
      "1.5356089\n",
      "2.7912278\n",
      "1.5329586\n",
      "2.7892594\n",
      "1.530353\n",
      "2.7872953\n",
      "1.527791\n",
      "2.7853332\n",
      "1.5252723\n",
      "2.7833748\n",
      "1.5227956\n",
      "2.7814183\n",
      "1.5203598\n",
      "2.7794635\n",
      "1.5179638\n",
      "2.7775095\n",
      "1.5156076\n",
      "2.7755563\n",
      "1.5132896\n",
      "2.7736049\n",
      "1.5110096\n",
      "2.7716525\n",
      "1.5087665\n",
      "2.7697005\n",
      "1.50656\n",
      "2.7677476\n",
      "1.5043881\n",
      "2.7657955\n",
      "1.5022516\n",
      "2.763842\n",
      "1.500149\n",
      "2.761888\n",
      "1.4980803\n",
      "2.7599335\n",
      "1.4960439\n",
      "2.757979\n",
      "1.4940395\n",
      "2.7560232\n",
      "1.492067\n",
      "2.7540667\n",
      "1.4901255\n",
      "2.7521098\n",
      "1.4882143\n",
      "2.7501519\n",
      "1.4863325\n",
      "2.7481933\n",
      "1.4844799\n",
      "2.746235\n",
      "1.4826558\n",
      "2.7442753\n",
      "1.4808598\n",
      "2.7423158\n",
      "1.4790908\n",
      "2.7403553\n",
      "1.4773493\n",
      "2.7383945\n",
      "1.4756337\n",
      "2.7364345\n",
      "1.4739447\n",
      "2.7344737\n",
      "1.4722807\n",
      "2.732514\n",
      "1.4706417\n",
      "2.730554\n",
      "1.4690268\n",
      "2.728595\n",
      "1.4674358\n",
      "2.7266366\n",
      "1.4658686\n",
      "2.7246795\n",
      "1.4643241\n",
      "2.722724\n",
      "1.4628022\n",
      "2.7207692\n",
      "1.4613022\n",
      "2.718816\n",
      "1.4598237\n",
      "2.7168643\n",
      "1.4583668\n",
      "2.7149138\n",
      "1.4569299\n",
      "2.7129662\n",
      "1.4555137\n",
      "2.7110205\n",
      "1.4541172\n",
      "2.7090764\n",
      "1.4527398\n",
      "2.707135\n",
      "1.4513818\n",
      "2.7051966\n",
      "1.4500424\n",
      "2.703261\n",
      "1.448721\n",
      "2.7013276\n",
      "1.4474175\n",
      "2.6993983\n",
      "1.4461311\n",
      "2.6974716\n",
      "1.4448625\n",
      "2.6955485\n",
      "1.4436104\n",
      "2.693629\n",
      "1.4423743\n",
      "2.6917129\n",
      "1.4411538\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(50000):\n",
    "        for iteration, example  in enumerate(examples):\n",
    "            x_ = np.vstack((np.zeros(len(char_to_ix)),word_to_one_vector(example, char_to_ix)))\n",
    "            x_ = x_.reshape(1, x_.shape[0], x_.shape[1])\n",
    "            y_ = np.append(np.asarray([(char_to_ix[x]) for x in example]),0)\n",
    "            y_ = y_.reshape(1, y_.shape[0])\n",
    "            sess.run(training_op, feed_dict = {X: x_, y: y_, seq_len: x_.shape[1]})\n",
    "            #out = sess.run(outputs_2, feed_dict = {X: x_, y: y_, seq_len: x_.shape[1]})\n",
    "            #print(out.shape, y_)\n",
    "            if iteration % 1000 == 0:\n",
    "                print(loss.eval(feed_dict = {X: x_, y: y_, seq_len: x_.shape[1]}))\n",
    "                #out = sess.run(outputs_2, feed_dict = {X: np.zeros((10,len(char_to_ix))).reshape(1,10,len(char_to_ix)), seq_len: 10})\n",
    "                #indexs = tf.argmax(tf.nn.softmax(out), 1).eval()[0]\n",
    "                #word = ''.join([ ix_to_char[x] for x in indexs])\n",
    "                #print(word)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
