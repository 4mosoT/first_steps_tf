{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "#https://gist.github.com/danijar/d11c77c5565482e965d1919291044470\n",
    "#https://github.com/crestonbunch/neural-namer/blob/master/modeler/network.py\n",
    "#https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "#https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 29 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data)) + ['INIT', '@']\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: '@', 2: 'INIT', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'x', 27: 'y', 28: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_one_vector(word, char_to_ix, paddTo = 0):\n",
    "    one_hot_vectors = np.zeros((len(word) + 1, len(char_to_ix)))\n",
    "    one_hot_vectors[ np.arange(len(word)) + 1, [char_to_ix[ch] for ch in word] ]= 1\n",
    "    one_hot_vectors[0, 1] = 2\n",
    "    if paddTo > 0 and paddTo > one_hot_vectors.shape[0]:\n",
    "        padding = np.zeros((paddTo - one_hot_vectors.shape[0] , len(char_to_ix)))\n",
    "        #padding[0, 1] = 1\n",
    "        one_hot_vectors = np.vstack((one_hot_vectors, padding))\n",
    "    return one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dinos.txt\") as f:\n",
    "    examples = f.readlines()\n",
    "examples = [x.lower().strip() for x in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for x in examples:\n",
    "    if len(x) > max_length:\n",
    "        max_length = len(x)\n",
    "max_length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(word_to_one_vector(examples[0],char_to_ix, 27), examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_examples = np.empty((len(examples), max_length , vocab_size))\n",
    "y_examples = np.empty((len(examples), max_length ))\n",
    "lengths = []\n",
    "for iter_number, example  in enumerate(examples):\n",
    "    lengths.append(len(example) + 1)\n",
    "    x_ = np.vstack((word_to_one_vector(example, char_to_ix, max_length)))\n",
    "    X_examples[iter_number] = x_\n",
    "    y_ = np.asarray([(char_to_ix[x]) for x in example + \"@\"])\n",
    "    y_ = np.append(y_, np.zeros((max_length - 1 - len(example))))\n",
    "    y_examples[iter_number] = y_\n",
    "#print(X_examples[0], y_examples[0], lengths[0])\n",
    "#print(X_examples.shape, y_examples.shape, len(lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 100\n",
    "n_steps = max_length\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, None, vocab_size], name = \"inputs\")\n",
    "y = tf.placeholder(tf.int32, [None, n_steps], name = \"targets\") #Shape => Batch_Size x Steps\n",
    "seq_len = tf.placeholder(tf.int32, [None], name = \"seq_lentgh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units =  n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype = tf.float32, sequence_length = seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs = tf.reshape(outputs, [-1, n_neurons])\n",
    "stacked_outputs_dense = tf.layers.dense(stacked_outputs, vocab_size)\n",
    "outputs_2 = tf.reshape(stacked_outputs_dense, [-1, n_steps, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = outputs_2)\n",
    "mask = tf.cast(tf.sign(y), tf.float32 ) \n",
    "xentropy *= mask\n",
    "xentropy = tf.reduce_sum(xentropy, reduction_indices = 1)\n",
    "xentropy /= tf.cast(seq_len, tf.float32)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.1610544\n",
      "ggomrhqrwqdts\n",
      "nnegf\n",
      "ogmbxmf\n",
      "ucd\n",
      "rqrzlzcvmrpjhfINITufkguaerovzwfmknqygoqaayb\n",
      "rronmrgzogwqoun\n",
      "\n",
      "\n",
      "50 2.4133966\n",
      "daorsaurus\n",
      "eerktorau\n",
      "angorausurs\n",
      "ngdons\n",
      "segiataus\n",
      "cisoraus\n",
      "\n",
      "\n",
      "100 2.2544274\n",
      "gelanedoe\n",
      "saur\n",
      "eus\n",
      "ados\n",
      "aiang\n",
      "ppynanus\n",
      "\n",
      "\n",
      "150 2.1395946\n",
      "lonharbanus\n",
      "anksaurus\n",
      "amals\n",
      "uosaurus\n",
      "aliptcona\n",
      "laitosaurus\n",
      "\n",
      "\n",
      "200 2.0606134\n",
      "crgisaurus\n",
      "oeiia\n",
      "tanclhisaur\n",
      "anooertis\n",
      "opisaurus\n",
      "mruosauru\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20000):\n",
    "        for batch in range(len(examples)// batch_size):\n",
    "            \n",
    "            X_batch = X_examples[batch * batch_size: (batch + 1) * batch_size]\n",
    "            y_batch = y_examples[batch * batch_size: (batch + 1) * batch_size]\n",
    "            batch_lengths = lengths[batch * batch_size: (batch + 1) * batch_size]\n",
    "            batch_dict = {X: X_batch, y: y_batch, seq_len: batch_lengths}\n",
    "            sess.run(training_op, feed_dict = batch_dict)\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            print(epoch, loss.eval(feed_dict = batch_dict))    \n",
    "            for x in range(6):\n",
    "                word = []\n",
    "                x = np.zeros((n_steps,vocab_size)).reshape(-1, n_steps, vocab_size)\n",
    "                x[0, np.arange(x.shape[1]),1] = 1\n",
    "               \n",
    "                for iter_number in range(40):\n",
    "                    out = sess.run(outputs_2, feed_dict = {X: x[:,-n_steps:,:], seq_len: [n_steps]})\n",
    "                    last = out[0,-1,:]\n",
    "                    last_softmax = tf.nn.softmax(last).eval()\n",
    "\n",
    "                    choice = np.random.choice(range(vocab_size), p = last_softmax)\n",
    "\n",
    "                    one_hot = np.zeros(vocab_size)\n",
    "                    one_hot[choice] = 1\n",
    "\n",
    "                    x = np.append(x, one_hot.reshape(1,1,-1), axis=1)\n",
    "                    if choice == 1:\n",
    "                        break\n",
    "                    word.append(ix_to_char[choice])\n",
    "                word = ''.join(word) \n",
    "                print(word)\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
